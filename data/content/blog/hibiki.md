---
title: Hibiki：高保真同步语音到语音翻译
publishDate: 2025-02-09
description: Hibiki (“echo” in Japanese), a system for streaming and expressive speech-to-speech (S2ST) and speech-to-text (S2TT) translation
---


想象一下，一位国际会议的同声传译员，在演讲者话音未落之时，便能流畅自然地用另一种语言同步表达演讲内容，实现无缝的跨语言沟通。这种如同“回声”一般即时且准确的翻译体验，正是我们今天要介绍的 Hibiki (日语中“回声”之意) 技术的追求。

Hibiki 是一种解码器模型，专为同声传译任务而设计。它创新性地利用多流语言模型，同步处理源语言和目标语言的语音，并联合生成文本和音频 tokens，从而实现语音到文本 (S2TT) 和语音到语音 (S2ST) 的双重翻译能力。

### 同步翻译的挑战与 Hibiki 的应对

与传统的离线翻译（模型在获取完整源语句子后才开始翻译）不同，同步翻译面临着更为严峻的挑战。它要求模型像人类同声传译员一样，实时决策，判断何时积累了足够的上下文信息，从而开始生成准确的翻译，并且需要逐块、逐步地进行翻译。

为了解决同步翻译的难题，Hibiki 引入了一种弱监督方法。该方法巧妙地利用现成的文本翻译系统的困惑度，来识别每个词语的最佳延迟时间，并以此创建对齐的合成数据。通过对这些数据进行监督学习，Hibiki 能够执行自适应的同声传译，并且在推理时仅需使用简单的温度采样技术。

### Hibiki 的卓越性能

在法语-英语同声传译任务中，Hibiki 展示了最先进的性能，在翻译质量、说话人保真度和自然度方面均超越了以往的模型。更令人惊喜的是，Hibiki 的推理过程非常简洁，这使得它能够兼容批量翻译，甚至可以实时部署在设备端。

为了让大家更直观地感受 Hibiki 的魅力，作者提供了示例演示 以及模型和推理代码。

### Hibiki模型细节

Hibiki 的核心在于其多流架构，它能够同步接收源语言语音并生成目标语言语音。这种架构基于嵌套的全局和局部 Transformer，能够联合建模两个音频流，并通过预测分层的文本和音频 tokens 来完成翻译任务。

#### 1. Neural Audio Codec (Mimi Codec)

Hibiki 采用了预训练的因果流式 Mimi 编解码器，将源语言和目标语言的语音编码成低帧率的离散 tokens 序列。Mimi 包含一个编码器和一个解码器，以及一个使用残差向量量化（RVQ）的信息瓶颈。

编码器将持续时间为 d 的输入波形转换为维度为 C，帧率为 fr（12.5 Hz）的潜在向量 U。然后，U 被投影到码本表中，并重复此过程 Q 次以获得更精细的量化表示。解码器则被训练为从投影张量的总和中重建输入波形。

对于语言建模，我们更关注码本条目的离散索引，即 (At,q)。按照惯例，第一个量化级别的输出 (At,1) 被称为语义 tokens，而 (At,q≥2) 则被称为声学 tokens。声学 tokens 以粗到精的方式排列，前者捕捉音频的主要特征，后者则精细地建模音频细节，确保平滑的感知效果。

#### 2. 离散音频 Tokens 的联合建模

为了有效地建模离散音频 tokens，Hibiki 利用了 RQ-Transformer。它包含一个大型的时间 Transformer，以与编解码器相同的帧率 fr 运行，并接收到目前为止生成的所有 tokens。时间 Transformer 的输出 Zt 随后被输入到一个更小规模的 深度 Transformer 中，深度 Transformer 自回归地建模量化器轴上的 tokens (At,1, ..., At,Q)。

为了引入声学延迟，模型实际上建模的是 (τ(A)t,q) 而不是 A。这种延迟在解码音频之前被移除。

#### 3. 多流建模实现翻译

Hibiki 将目标语言解释 Y 的音频 tokens (AY) 与源语言语句 X 的 tokens (AX) 沿 q 轴连接起来，形成一个联合的 tokens 序列 Ā。虽然在推理时跳过了对源语言 tokens 的预测，但在训练时建模 AX 被证明是有益的。

与 [Defossez et al. 2024]  类似，Hibiki 还预测一个 Inner Monologue，即与生成音频内容对齐的填充文本 tokens 流 (Wt)。这种文本流充当语音生成的支架，并在推理时被积极使用。

#### 4. 架构细节

Hibiki 的时间 Transformer 具有 2560 的潜在维度，24 层，以及每头 128 维。深度 Transformer 初始版本每码本 6 层，潜在维度 1024，每头 64 维，建模输出流的 Q=16 音频码本和输入流的相同数量的码本（仅在训练时）。为了减小深度 Transformer 的尺寸，Hibiki 采用了后训练蒸馏技术，将其缩减为一个更小的版本，参数量从 1.1B 减少到 449M。

### 上下文对齐与合成解释数据

Hibiki 的核心创新之一是上下文对齐方法，它用于估计和加强源语言和目标语言语句之间的因果依赖关系，从而实现同步翻译。

#### 1.  文本域对齐

上下文对齐旨在找到理想的对齐方式 (a<sup>ideal</sup><sub>j</sub>)，指示目标语言中第 j 个词 (T<sub>j</sub>) 应该等待源语言中哪个词 (S<sub>a<sup>ideal</sup><sub>j</sub></sub>)，以最大程度地减少对 T<sub>j</sub> 的不确定性。

为了估计理想对齐，Hibiki 引入了上下文对齐标准。它利用预训练的文本翻译语言模型 MADLAD-3B 计算条件对数似然 log(p<sub>j,i</sub>)，并假设当 i = a<sub>j</sub> 时，增量对数似然 δ<sub>j,i</sub> = log(p<sub>j,i</sub>) − log(p<sub>j,i-1</sub>) 最大。通过这种方式，Hibiki 可以推导出上下文对齐 (a<sup>ctx</sup><sub>j</sub>)。

#### 2.  音频域对齐

在音频域中，Hibiki 使用 Whisper 模型转录源语言和目标语言的语音，并计算文本域的上下文对齐。如果目标语言语音 (Y) 不符合对齐，则通过在词语前插入足够的静音来调整 Y，或者使用对齐感知 TTS 模型重新合成 Y 以获得更自然的对齐数据。

### 语音迁移与条件训练

为了提升语音迁移效果，Hibiki 采用了条件训练。模型根据说话人相似度的分位数，将每个训练样本标记为不同的**语音迁移分数**类别（例如，非常好、好、中性、差、非常差），并将与每个类别相关的可学习嵌入添加到模型输入中。在推理时，始终传递“非常好”的标签。

此外，Hibiki 还采用了 Classifier-Free Guidance 技术，通过结合有条件和无条件 logits 进行采样，进一步增强了条件作用的影响，显著提高了语音迁移效果。

### 实验结果

Hibiki 在法语-英语语音翻译任务中进行了广泛的实验评估，并与离线基线模型以及 Seamless 和 StreamSpeech 等同步翻译模型进行了对比。

实验结果表明，Hibiki 在翻译质量 (ASR-BLEU) 上超越了所有离线基线模型。与同步翻译模型相比，Hibiki 在短句和长句翻译任务中均展现出卓越的性能，尤其是在 BLEU 分数、说话人相似度和 End Offset 延迟方面。人类评估也表明，Hibiki 在自然度和音频质量方面显著优于 Seamless，并且在同声传译体验上接近专业人类水平。

### 总结与展望

Hibiki 的出现，为同声传译领域带来了突破性的进展。它不仅在翻译质量、语音保真度和自然度方面达到了新的高度，更重要的是，其简洁的推理过程使其具备了实时性和设备端部署的潜力。

Hibiki 的成功，归功于其创新的多流架构、上下文对齐方法以及条件训练策略。这些技术的结合，使得 Hibiki 能够像人类同声传译员一样，实时、自适应地进行高质量的语音翻译，为未来的跨语言交流应用开辟了广阔的前景。

我们相信，随着 Hibiki 代码、模型和数据集的开源，将会有更多的研究人员和开发者加入到同声传译的研究和应用中来，共同推动这一领域的快速发展。

参考链接

Hibiki 示例演示: https://hf.co/spaces/kyutai/hibiki-samples

Hibiki 模型和推理代码: https://github.com/kyutai-labs/hibiki